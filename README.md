# Agentic TSP Solver
![alt text](https://img.shields.io/badge/python-3.10+-blue.svg)
![alt text](https://img.shields.io/badge/License-MIT-yellow.svg)

This repository provides a framework for exploring a key question in modern AI: **Can a Large Language Model (LLM) effectively collaborate with classical algorithms to solve a complex optimization problem?**

We use the well-known Traveling Salesperson Problem (TSP) as a testbed. The system implements a hybrid, two-stage agentic architecture to find a solution for the berlin52 benchmark problem, where the optimal tour is known.

## Core Concept: A Hybrid Agentic Architecture

The solver does not rely on a single method. Instead, it orchestrates a collaboration between two different types of agents in a two-stage process:

### Stage 1: Classical Optimization (The Workhorses)
A pool of independent Genetic Algorithm (GA) workers runs concurrently. Each worker evolves a population of solutions over hundreds of generations, applying principles of selection, crossover, and mutation. This stage performs the heavy computational lifting to find a strong, but not necessarily perfect, candidate solution.

### Stage 2: LLM Refinement (The Heuristic Expert)
The single best tour found by the classical workers in Stage 1 is handed off to an LLM agent (gpt-3.5-turbo). The LLM is prompted with the tour, the city coordinates, and context on TSP heuristics (like uncrossing paths via 2-opt). Its task is to analyze the tour and propose a refinement based on this high-level guidance.

This architecture allows us to measure the value added by the LLM when it attempts to improve upon the work of a powerful, specialized algorithm.

## Getting Started

Follow these steps to set up and run the benchmark on your own machine.

### 1. Installation

First, clone the repository and navigate into the project directory.

```bash
git clone git@github.com:lbrichards/agentic_tsp.git
cd agentic_tsp
```

Next, it is highly recommended to create and activate a Python virtual environment.

```bash
# Create the virtual environment
python -m venv venv

# Activate it (on macOS/Linux)
source venv/bin/activate

# Or on Windows
# venv\Scripts\activate
```

Finally, install the required dependencies.

```bash
pip install -r requirements.txt
```

### 2. API Key Configuration

The system requires an OpenAI API key to communicate with the LLM agent. The project uses a `.env` file to manage this key securely.

A helper script is provided to set this up. Run the following command and you will be prompted to paste your API key:

```bash
python src/setup_env.py
```

This will create a `.env` file in the project root. This file is explicitly ignored by `.gitignore` and should never be committed to version control.

### 3. Running the Benchmark

With the setup complete, you can run the full benchmark experiment with a single command:

```bash
python -m src.agentic_tsp.main
```

The script will execute 5 full runs of the hybrid orchestration process. It will print the progress of each run and conclude with a final, aggregated benchmark report.

## Empirical Results: The Benchmark

The primary goal of this repository is to establish a clear, unequivocal benchmark for this specific collaborative pattern. The following report was generated by running the system as configured.

### Token-Based LLM Collaboration Benchmark Report

**Benchmark:** berlin52 (Optimal Length: 7542.00)  
**Total Benchmark Iterations:** 5  
**Workers per Iteration:** 10  

#### Performance Statistics (lower is better)

|                    | Classical GA | LLM-Refined |
|--------------------|-------------|-------------|
| Average Tour Length | 8026.85    | 8041.81     |
| Best Tour Length   | 7904.52     | 7904.52     |
| Std Dev of Length  | 87.87       | 92.11       |

#### Cost Statistics
**Average Computation Time per Run:** 25.49 seconds

## Conclusion

The results provide a clear benchmark. The classical Genetic Algorithm consistently found a highly optimized solution, on average reaching within ~6.5% of the known optimum.

In this configuration, the LLM agent—prompted with a 2-opt heuristic via a natural language channel—did not reliably improve upon this already strong baseline. The final average tour length was slightly higher, as was the solution variance. This establishes a robust performance baseline for this collaborative architecture.

## Contributing

This framework is designed for experimentation. Feel free to fork the repository and explore modifications:

- Try different LLMs (e.g., gpt-4o, claude-3-opus).
- Engineer more advanced prompts in `src/agentic_tsp/llm_worker.py`.
- Tune the Genetic Algorithm parameters in `src/agentic_tsp/orchestrator.py`.

## License

This project is licensed under the MIT License.